{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from slackclient import SlackClient\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from time import strftime, gmtime, localtime\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "import numpy as np\n",
    "from random import randint\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SLACK_BOT_TOKEN='SLACK_BOT_TOKEN'\n"
     ]
    }
   ],
   "source": [
    "# Slack bot setting up and starter code: https://github.com/mattmakai/slack-starterbot/blob/master/starterbot.py\n",
    "%env SLACK_BOT_TOKEN='SLACK_BOT_TOKEN'\n",
    "# instantiate Slack client\n",
    "slack_client = SlackClient(os.environ.get('SLACK_BOT_TOKEN'))\n",
    "# starterbot's user ID in Slack: value is assigned after the bot starts up\n",
    "starterbot_id = None\n",
    "\n",
    "# constants\n",
    "RTM_READ_DELAY = 1 # 1 second delay between reading from RTM\n",
    "EXAMPLE_COMMAND = \"do\"\n",
    "MENTION_REGEX = \"^<@(|[WU].+?)>(.*)\"\n",
    "\n",
    "\n",
    "\n",
    "#print(SLACK_BOT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection failed. Exception traceback printed above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/anon/SlackBot/slackbot1/lib/python3.6/site-packages/slackclient/client.py\", line 52, in rtm_connect\n",
      "    self.server.rtm_connect(use_rtm_start=with_team_state, **kwargs)\n",
      "  File \"/Users/anon/SlackBot/slackbot1/lib/python3.6/site-packages/slackclient/server.py\", line 151, in rtm_connect\n",
      "    raise SlackLoginError(reply=reply)\n",
      "slackclient.server.SlackLoginError\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if slack_client.rtm_connect(with_team_state=False):\n",
    "        print(\"Starter Bot connected and running!\")\n",
    "        # Read bot's user ID by calling Web API method `auth.test`\n",
    "        starterbot_id = slack_client.api_call(\"auth.test\")[\"user_id\"]\n",
    "        \n",
    "        while True:\n",
    "            command, channel = parse_bot_commands(slack_client.rtm_read())\n",
    "            if command:\n",
    "                handle_command(command, channel)\n",
    "            time.sleep(RTM_READ_DELAY)\n",
    "    else:\n",
    "        print(\"Connection failed. Exception traceback printed above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pattern_response.json') as json_data:\n",
    "    conversation = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification code - https://github.com/ugik/notebooks/blob/master/Neural_Network_Classifier.ipynb\n",
    "words = []\n",
    "documents = []\n",
    "classes = []\n",
    "responses = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "#Cycling through each item - representing a tag-pattern-response in the json document)\n",
    "for tags in conversation['converse_intents']:\n",
    "\n",
    "#cycling through each sentence in a pattern (one pattern is a group of sentences) in the \n",
    "#item extracted in the first loop\n",
    "    \n",
    "    for patterns in tags['patterns']:\n",
    "\n",
    "#tokenizing the sentences in each pattern - basically splits into individual words\n",
    "        w = nltk.word_tokenize(patterns)\n",
    "\n",
    "#adding (extending) to the words array\n",
    "        words.extend(w)\n",
    "        \n",
    "#Combining the tokenized word(s) and the class it belongs to - which is represented by the 'tag' in the json doc \n",
    "#Useful to know the difference between append and extend\n",
    "        documents.append((w, tags['tag']))\n",
    "\n",
    "#Adding each of these 'tag' to the classes array - this is done outside of the inner loop to prevent duplication\n",
    "#as there can be multiple patterns for each tag\n",
    "    if tags['tag'] not in classes:\n",
    "        classes.append(tags['tag'])\n",
    "    \n",
    "    if tags['responses'] not in responses:\n",
    "        responses.append(tags['responses'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The words are stemmed - basically trimming to their roots - sometimes does not make much sense\n",
    "\n",
    "words = [stemmer.stem(word.lower()) for word in words if word not in ignore_words]\n",
    "\n",
    "#set- uniques the list of words generated\n",
    "words = list(set(words))\n",
    "\n",
    "#print(len(words))\n",
    "#print(len(classes))\n",
    "#print(len(documents))\n",
    "#print(words)\n",
    "#print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing some arrays\n",
    "binary_array = []\n",
    "output_data = []\n",
    "output_empty = [0]*len(classes)\n",
    "\n",
    "#cycling through each of the 27 documents - a document is a tokenized pattern combined with its class\n",
    "#enumerate helps in iterating a counter for each cycle of the loop\n",
    "\n",
    "for i,item in enumerate(documents):\n",
    "    \n",
    "#extracts only the tokenized pattern leaving out the class    \n",
    "    sentence = item[0]\n",
    "\n",
    "#Now stemming the words in this tokenized pattern     \n",
    "    unit_words = [stemmer.stem(word.lower()) for word in sentence]\n",
    "    bag = []\n",
    "    \n",
    "\n",
    "#Now taking each of the word from the unique stemmed words done earlier and checking if it is in the stemmed tokenized \n",
    "#pattern extracted in the outer loop. if present, the bag array is appended by a 1 or a 0. Basically, we are taking \n",
    "# the uniques stemmed words made earlier and checking every time with the stemmed tokenized pattern and producing a 1 \n",
    "# for a match\n",
    "\n",
    "    for string in words: \n",
    "        if (string in unit_words):\n",
    "            bag.append(1)\n",
    "            \n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "#the second part of the document which is the class is taken and its index is found from the classes list created\n",
    "#earlier and that particular row element is given a 1. For instance if the class index is 8, then the 8th element in \n",
    "# row is assigned 1. And a final array has the combination of the bag and the output.\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(item[1])]=1\n",
    "    binary_array.append(bag)\n",
    "    output_data.append(output_row)\n",
    "\n",
    "input_array = np.array(binary_array)\n",
    "output_array = np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_input_text(input):\n",
    "    ignore_words = ['?']\n",
    "    input_bag = []\n",
    "    \n",
    "    input_token = nltk.word_tokenize(input)\n",
    "    input_stem = [stemmer.stem(word.lower()) for word in input_token if word not in ignore_words]\n",
    "    \n",
    "    for string in words:\n",
    "        if string in input_stem:\n",
    "            input_bag.append(1)\n",
    "        else:\n",
    "            input_bag.append(0)\n",
    "    \n",
    "    input_bag = np.array(input_bag)\n",
    "    match = np.dot(input_array,input_bag)\n",
    "    \n",
    "    match_class = match.argmax(axis=0)\n",
    "    \n",
    "    response_string = handling_response(output_array[match_class].argmax(axis=0))\n",
    "    return(response_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_response(class_index):\n",
    "    response_counter = [0] * len(classes)\n",
    "    nthresponse = randint(0,(len(responses[class_index])-1))\n",
    "    output_response = responses[class_index][nthresponse]\n",
    "    return(output_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bot_commands(slack_events):\n",
    "    \"\"\"\n",
    "        Parses a list of events coming from the Slack RTM API to find bot commands.\n",
    "        If a bot command is found, this function returns a tuple of command and channel.\n",
    "        If its not found, then this function returns None, None.\n",
    "    \"\"\"\n",
    "    for event in slack_events:\n",
    "        if event[\"type\"] == \"message\" and not \"subtype\" in event:\n",
    "            user_id, message = parse_direct_mention(event[\"text\"])\n",
    "            if user_id == starterbot_id:\n",
    "                return message, event[\"channel\"]\n",
    "    return None, None\n",
    "\n",
    "def parse_direct_mention(message_text):\n",
    "    \"\"\"\n",
    "        Finds a direct mention (a mention that is at the beginning) in message text\n",
    "        and returns the user ID which was mentioned. If there is no direct mention, returns None\n",
    "    \"\"\"\n",
    "    matches = re.search(MENTION_REGEX, message_text)\n",
    "    # the first group contains the username, the second group contains the remaining message\n",
    "    return (matches.group(1), matches.group(2).strip()) if matches else (None, None)\n",
    "\n",
    "def handle_command(command, channel):\n",
    "    \"\"\"\n",
    "        Executes bot command if the command is known\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default response is help text for the user\n",
    "    default_response = \"Not sure what you mean. Try *{}*.\".format(EXAMPLE_COMMAND)\n",
    "\n",
    "    # Finds and executes the given command, filling in response\n",
    "    response = None\n",
    "    # This is where you start to implement more commands!\n",
    "    \n",
    "    #if command.startswith(EXAMPLE_COMMAND):\n",
    "     #   response = \"Sure...write some more code then I can do that!\"\n",
    "    \n",
    "    time_check = re.compile('time')\n",
    "    weather_check = re.compile('weather')\n",
    "    if time_check.search(command):\n",
    "        response = \"The time now is %s\" %strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n",
    "    elif weather_check.search(command):\n",
    "        response = requests.get(\"http://api.openweathermap.org/data/2.5/weather?id=1880252&APPID=e58f9485573ee17e2ecff9a313420bcf\").json()\n",
    "    else:\n",
    "        response = handling_input_text(command)\n",
    "         \n",
    "    \n",
    "    \n",
    "    # Sends the response back to the channel\n",
    "    slack_client.api_call(\n",
    "        \"chat.postMessage\",\n",
    "        channel=channel,\n",
    "        text=response or default_response\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
